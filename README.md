
---

# Scriptify AI ‚Äî Sistema de Recomendaci√≥n de Contenidos (RAG + LLM)

Motor de recomendaciones para **ideas de contenido** y **gu√≠as accionables** orientadas a redes (YouTube/Shorts/TikTok/IG).
Combina **RAG** con un **grafo Neo4j** (nichos, ejemplos reales, tendencias) y un **LLM** (Ollama v√≠a LlamaIndex) para producir:

* `recommendation`: 1 frase clara que incluya alguna *specialty*.
* `reason`: p√°rrafo con analog√≠a original + **exactamente 4 bullets** accionables (formato `- `).
* `ideas`: 10‚Äì12 t√≠tulos √∫tiles (mix directos/creativos).
* `hashtags_for_ideas`: 2‚Äì3 hashtags por idea (limpios, sin tildes ni gen√©ricos).
* Opcional: `examples` y `trends` derivados del grafo (si hay datos).

---

## üß± Arquitectura

* **FastAPI** (API REST)
* **Ollama + LlamaIndex** (LLM local o remoto)
* **Neo4j** (grafo con `Niche`, `Example`, `Keyword` y relaciones como `BELONGS_TO`, `HAS_TAG`, `IN_TREND`)
* **RAG** en `services/graph_examples.py` ‚Üí construye contexto (glosario, trends, ejemplos)
* **Saneado/validaci√≥n** en `services/llm_ollama.py` ‚Üí corrige spanglish, hashtags vac√≠os, bullets, etc.

```
client ‚Üí FastAPI (/recommend/llm) ‚Üí LlamaIndex(Ollama)
                          ‚Üò
                           Neo4j (examples + trends ‚Üí vocab/hashtags)
```

---

## üöÄ Inicio r√°pido

### 1) Requisitos

* Docker + Docker Compose
* (Opcional) Python 3.11+ si quieres ejecutar local sin contenedores
* **Neo4j** con APOC habilitado y datos de `Niche/Example/Keyword`
* **Ollama** con un modelo disponible (ej. `llama3.1` o el que uses)

### 2) Variables de entorno

Crea un `.env` (o exporta en tu shell) con algo como:

```env
API_KEY=supersecreto
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=pass
NEO4J_DB=neo4j

# Direcci√≥n del servidor de Ollama al que se conectar√° la API
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=llama3.1:latest
LLM_TIMEOUT=60
```

> Si corres Ollama fuera de Docker en tu m√°quina: `OLLAMA_HOST=http://host.docker.internal:11434`.

### 3) Docker Compose

```bash
docker compose up -d --build
docker compose logs -f api
```

Ver√°s:

```
Uvicorn running on http://0.0.0.0:8000
```

---

## üìö Endpoints

### üîπ `POST /recommend/llm`

Genera recomendaci√≥n + ideas. **Requiere** header `x-api-key`.

**Headers**

```
x-api-key: supersecreto
Content-Type: application/json
```

**Request (ejemplo Tecnolog√≠a + XR)**

```json
{
  "platform": "youtube",
  "niche": "tecnologia",
  "impressions": 90000,
  "reach": 72000,
  "clicks": 880,
  "followers": 41000,
  "likes": 2800,
  "shares": 210,
  "saves": 360,
  "comments": 230,
  "specialties": ["realidad virtual", "realidad mixta", "webxr", "xr"],
  "use_graph": true,
  "top_k": 8
}
```

> **Nota**: En la UI el usuario no introduce `avg_watch_pct` ni `completion_rate`. Si tu backend los necesita, usa defaults.

**cURL**

```bash
curl -sS -X POST "http://localhost:8000/recommend/llm?pretty=1" \
  -H "x-api-key: supersecreto" -H "Content-Type: application/json" \
  --data-binary @/tmp/req_tecnologia_xr.json | jq .
```

**Response (resumen)**

```json
{
  "recommendation": "Realidad mixta para retener: explora los mejores casos de uso.",
  "reason": "Se√±ales: ... - bullet1 - bullet2 - bullet3 - bullet4",
  "ideas": ["...", "..."],
  "hashtags_for_ideas": [["#realidadmixta","#webxr"], ...],
  "examples": [
    {"title": "...", "videoId": "...", "publishedAt": "2025-10-10T15:00:54+00:00", "hashtags_for_examples": ["#..."] }
  ],
  "diagnostics": { "inputs": { ... }, "llm": true }
}
```

### üîπ `POST /feedback/like`

Registra un ‚Äúme gusta‚Äù de una **idea** para el nicho (puedes usarlo para reforzar tendencias futuras).

**Body**

```json
{
  "niche": "tecnologia",
  "region": "GL",
  "idea": "Comparativa: Realidad Virtual vs Realidad Mixta",
  "specialties": ["realidad virtual", "realidad mixta"]
}
```

**cURL**

```bash
curl -sS -X POST "http://localhost:8000/feedback/like" \
  -H "x-api-key: supersecreto" -H "Content-Type: application/json" \
  --data-binary @/tmp/feedback_like.json | jq .
```

---

## üß† Contexto RAG desde el grafo

El servicio intenta traer de Neo4j:

* **examples** (t√≠tulos y hashtags limpios)
* **trends** (palabras clave con score)

Ambos alimentan:

* `glossary` (a partir de t√≠tulos)
* `trends_tokens` (bias de hashtags/t√≠tulos)
* Vocabulario permitido para **hashtags** (evita gen√©ricos y ruido)

### Ejemplo de Cypher (hashtags limpios por ejemplo)

```cypher
WITH [
  'the','and','for','con','por','las','los','una','unos','unas','que','como','para','consejos',
  'del','de','la','el','un','en','por','tu','sus','mis','muy','mas','m√°s','sin','son','soy',
  'esa','ese','esto','esta','este','cuando','donde','d√≥nde','qu√©','que','c√≥mo','cual','cu√°l',
  'sobre','entre','desde','hasta','solo','s√≥lo','todo','toda','todos','todas','aqui','aqu√≠',
  'ahora','hoy','pero','porque','porqu√©','ya','no','si','s√≠','al','lo','se','una','un','y','o'
] AS STOP

MATCH (n:Niche {name:'automotriz', region:'GL'})<-[:BELONGS_TO]-(e:Example)
OPTIONAL MATCH (e)-[:HAS_TAG]->(t:Tag)
WITH e, STOP, collect(DISTINCT toLower(t.name)) AS rawTags
WITH e, STOP, rawTags, apoc.text.split(toLower(coalesce(e.title,'')), '[^\\p{L}\\p{N}]+') AS words
WITH e, STOP, rawTags, words,
     [x IN rawTags | replace(replace(replace(replace(replace(replace(x,'√°','a'),'√©','e'),'√≠','i'),'√≥','o'),'√∫','u'),'√±','n')] AS tags_ascii,
     [x IN words   | replace(replace(replace(replace(replace(replace(x,'√°','a'),'√©','e'),'√≠','i'),'√≥','o'),'√∫','u'),'√±','n')] AS words_ascii
WITH e,
     [x IN tags_ascii  WHERE x <> '' AND size(x) >= 3 AND NOT x =~ '^[0-9].*'] AS tag_clean,
     [x IN words_ascii WHERE x <> '' AND size(x) >= 3 AND NOT x =~ '^[0-9].*'] AS word_clean
WITH e,
     [x IN tag_clean  | '#' + replace(x,' ','')] AS tag_hashes,
     [x IN word_clean | '#' + replace(x,' ','')] AS word_hashes
WITH e,
     CASE WHEN size(tag_hashes) > 0 THEN apoc.coll.toSet(tag_hashes)[..3]
          ELSE apoc.coll.toSet(word_hashes)[..3]
     END AS hashtags
RETURN
  e.videoId AS videoId,
  'https://youtu.be/' + e.videoId AS url,
  e.title AS title,
  e.publishedAt AS publishedAt,
  hashtags AS hashtags_for_examples
ORDER BY coalesce(e.publishedAt, datetime('1900-01-01')) DESC
LIMIT 15;
```

---

## üóÇÔ∏è Estructura del proyecto (resumen)

```
app/
  main.py                  # FastAPI (endpoints /recommend/llm, /feedback/like, etc.)
  services/
    graph_examples.py      # build_llm_context (glossary, trends, examples)
    llm_ollama.py          # prompt, chat, critic&repair, saneado/hashtags
    llamaindex_client.py   # get_llm() ‚Üí instancia de LlamaIndex con Ollama
infra/
  docker-compose.yml
  Dockerfile
README.md
```

---

## üß™ Ejemplos de requests

### Tecnolog√≠a + XR

```json
{
  "platform": "youtube",
  "niche": "tecnologia",
  "impressions": 90000,
  "reach": 72000,
  "clicks": 880,
  "followers": 41000,
  "likes": 2800,
  "shares": 210,
  "saves": 360,
  "comments": 230,
  "specialties": ["realidad virtual", "realidad mixta", "webxr", "xr"],
  "use_graph": true,
  "top_k": 8
}
```

### Automotriz (detailing)

```json
{
  "platform": "youtube",
  "niche": "automotriz",
  "impressions": 50000,
  "reach": 42000,
  "clicks": 400,
  "followers": 15000,
  "likes": 1200,
  "shares": 120,
  "saves": 210,
  "comments": 95,
  "specialties": ["detailing", "pulido", "interior"],
  "use_graph": true,
  "top_k": 8
}
```

---

## üõ†Ô∏è Desarrollo

### Levantar local (sin Docker)

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

Aseg√∫rate de tener **Ollama corriendo** y el **modelo descargado**:

```bash
ollama pull llama3.1
ollama serve
```

---

## ü©∫ Troubleshooting

* **`TypeError: Object of type DateTime is not JSON serializable`**
  Serializa `publishedAt` a string (ISO) *antes* de devolver JSON (ver `_coerce_examples` en `graph_examples.py`).

* **`NameError: name 're' is not defined`**
  Asegura `import re` en cualquier archivo que use regex (ej. funciones de tokenizaci√≥n en `main.py` o servicios).

* **`httpx.ReadTimeout: timed out` (Ollama)**

  * Verifica `OLLAMA_HOST` y que `ollama serve` est√© activo.
  * Sube `LLM_TIMEOUT`.
  * Comprueba que el `OLLAMA_MODEL` existe (`ollama list`).

* **`Neo.ClientNotification.Statement.FeatureDeprecationWarning` (CALL subquery)**
  No es fatal, pero puedes modernizar a `CALL (n) { ... }` para evitar deprecaciones futuras.

* **`jq: parse error: Invalid numeric literal`**
  Ocurre cuando la API responde con **texto** (p.ej., 500 con `text/plain`).
  Usa `curl -i` para ver `content-type` y el **status**:

  ```bash
  curl -i -sS -X POST "http://localhost:8000/recommend/llm" ... 
  ```

* **Hashtags vac√≠os o gen√©ricos**
  El saneador ahora rellena hasta **2‚Äì3 por idea** usando vocab permitido (glossary + specialties + trends).
  Evita `#tips`, `#checklist`, etc.

---

## üîí Seguridad

* Todas las rutas privadas requieren `x-api-key`.
* Evita exponer `NEO4J_*`, `OLLAMA_*` en logs p√∫blicos.
* Considera a√±adir **rate limit** y **CORS** seg√∫n tu despliegue.

---

## üó∫Ô∏è Roadmap

* [ ] Endpoints para **persistir feedback** (like/dislike) como sesgo de futuras tendencias.
* [ ] Soporte multi-plataforma (plantillas de estilo m√°s espec√≠ficas).
* [ ] Evaluadores autom√°ticos de calidad (detecci√≥n de clich√©s o repetici√≥n).
* [ ] M√©tricas de *post-hoc* (tracking de CTR/retenci√≥n por idea sugerida).

---

## üìÑ Licencia

Este proyecto se distribuye bajo la licencia **MIT**. (C√°mbiala si lo necesitas).

---

## üë§ Autor

**Jhonnatan Del Castillo** ‚Äî Lava Software Development
Emprendimientos IA, marketing digital y herramientas de recomendaci√≥n de contenido.

---
